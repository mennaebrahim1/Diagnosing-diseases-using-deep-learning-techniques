{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_body          6800\n",
      "a_body          6800\n",
      "category        6800\n",
      "q_body_count    6800\n",
      "a_body_count    6800\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\me513\\AppData\\Local\\Temp\\ipykernel_4088\\3672921815.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  squad['question']=squad['q_body']\n",
      "C:\\Users\\me513\\AppData\\Local\\Temp\\ipykernel_4088\\3672921815.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  squad['answers']=squad['a_body']\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "#read data\n",
    "data = pd.read_excel(r\"C:\\Users\\me513\\Downloads\\MAQA_30_words_Classifier_train_sorted.xlsx\")\n",
    "grouped_data = data.groupby('category')\n",
    "group_key ='امراض الدم'\n",
    "squad = grouped_data.get_group(group_key)\n",
    "print(squad.count())\n",
    "squad['question']=squad['q_body']\n",
    "squad['answers']=squad['a_body']\n",
    "# Step 2: Convert data to Hugging Face Dataset format\n",
    "dataset = Dataset.from_pandas(squad)\n",
    "# Drop the column(s) you want\n",
    "columns_to_drop = ['q_body', 'a_body','category','q_body_count','a_body_count']\n",
    "dataset = dataset.remove_columns(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(data)\n",
    "del(squad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "model_name ='AraBART-summ'"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e645e6f594b4c06b207fab34859c625",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\me513\\anaconda3\\envs\\gp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['question'], padding='max_length', truncation=True, max_length=30)\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['answers'], padding='max_length', truncation=True, max_length=30)\n",
    "        \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "537db90dd30d4370817b356a8a4a20e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5350 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 8.0381, 'grad_norm': 6.868765354156494, 'learning_rate': 2.5e-06, 'epoch': 0.05}\n",
      "{'loss': 8.0229, 'grad_norm': 6.269472122192383, 'learning_rate': 5e-06, 'epoch': 0.09}\n",
      "{'loss': 7.543, 'grad_norm': 4.581761360168457, 'learning_rate': 7.5e-06, 'epoch': 0.14}\n",
      "{'loss': 7.2786, 'grad_norm': 5.548696517944336, 'learning_rate': 1e-05, 'epoch': 0.19}\n",
      "{'loss': 7.293, 'grad_norm': 4.597141265869141, 'learning_rate': 1.25e-05, 'epoch': 0.23}\n",
      "{'loss': 6.9982, 'grad_norm': 4.438433647155762, 'learning_rate': 1.5e-05, 'epoch': 0.28}\n",
      "{'loss': 6.8831, 'grad_norm': 4.780729293823242, 'learning_rate': 1.75e-05, 'epoch': 0.33}\n",
      "{'loss': 6.2914, 'grad_norm': 3.422626495361328, 'learning_rate': 2e-05, 'epoch': 0.37}\n",
      "{'loss': 6.1312, 'grad_norm': 8.285125732421875, 'learning_rate': 2.25e-05, 'epoch': 0.42}\n",
      "{'loss': 6.0839, 'grad_norm': 5.246293544769287, 'learning_rate': 2.5e-05, 'epoch': 0.47}\n",
      "{'loss': 5.7652, 'grad_norm': 5.859610080718994, 'learning_rate': 2.7500000000000004e-05, 'epoch': 0.51}\n",
      "{'loss': 5.4481, 'grad_norm': 7.1115617752075195, 'learning_rate': 3e-05, 'epoch': 0.56}\n",
      "{'loss': 5.1403, 'grad_norm': 8.794987678527832, 'learning_rate': 3.2500000000000004e-05, 'epoch': 0.61}\n",
      "{'loss': 4.7609, 'grad_norm': 11.927175521850586, 'learning_rate': 3.5e-05, 'epoch': 0.65}\n",
      "{'loss': 4.402, 'grad_norm': 7.850345611572266, 'learning_rate': 3.7500000000000003e-05, 'epoch': 0.7}\n",
      "{'loss': 4.2532, 'grad_norm': 8.269208908081055, 'learning_rate': 4e-05, 'epoch': 0.75}\n",
      "{'loss': 3.9123, 'grad_norm': 2.427438735961914, 'learning_rate': 4.25e-05, 'epoch': 0.79}\n",
      "{'loss': 3.9098, 'grad_norm': 1.58339524269104, 'learning_rate': 4.5e-05, 'epoch': 0.84}\n",
      "{'loss': 3.7165, 'grad_norm': 1.3838717937469482, 'learning_rate': 4.75e-05, 'epoch': 0.89}\n",
      "{'loss': 3.8898, 'grad_norm': 1.4081240892410278, 'learning_rate': 5e-05, 'epoch': 0.93}\n",
      "{'loss': 3.6391, 'grad_norm': 1.827384114265442, 'learning_rate': 4.9952380952380954e-05, 'epoch': 0.98}\n",
      "{'loss': 3.7054, 'grad_norm': 1.323496699333191, 'learning_rate': 4.990476190476191e-05, 'epoch': 1.03}\n",
      "{'loss': 3.5887, 'grad_norm': 1.344211220741272, 'learning_rate': 4.985714285714286e-05, 'epoch': 1.07}\n",
      "{'loss': 3.7488, 'grad_norm': 1.304316759109497, 'learning_rate': 4.980952380952381e-05, 'epoch': 1.12}\n",
      "{'loss': 3.5218, 'grad_norm': 1.3341426849365234, 'learning_rate': 4.976190476190477e-05, 'epoch': 1.17}\n",
      "{'loss': 3.4585, 'grad_norm': 1.292985200881958, 'learning_rate': 4.971428571428572e-05, 'epoch': 1.21}\n",
      "{'loss': 3.6268, 'grad_norm': 1.2438642978668213, 'learning_rate': 4.966666666666667e-05, 'epoch': 1.26}\n",
      "{'loss': 3.4896, 'grad_norm': 1.2738149166107178, 'learning_rate': 4.961904761904762e-05, 'epoch': 1.31}\n",
      "{'loss': 3.4929, 'grad_norm': 1.2196259498596191, 'learning_rate': 4.957142857142857e-05, 'epoch': 1.36}\n",
      "{'loss': 3.5126, 'grad_norm': 1.4483497142791748, 'learning_rate': 4.9523809523809525e-05, 'epoch': 1.4}\n",
      "{'loss': 3.3483, 'grad_norm': 1.2916855812072754, 'learning_rate': 4.9476190476190476e-05, 'epoch': 1.45}\n",
      "{'loss': 3.2767, 'grad_norm': 1.237052321434021, 'learning_rate': 4.942857142857143e-05, 'epoch': 1.5}\n",
      "{'loss': 3.3297, 'grad_norm': 1.2582343816757202, 'learning_rate': 4.9380952380952386e-05, 'epoch': 1.54}\n",
      "{'loss': 3.4111, 'grad_norm': 1.2947694063186646, 'learning_rate': 4.933333333333334e-05, 'epoch': 1.59}\n",
      "{'loss': 3.3552, 'grad_norm': 1.2725623846054077, 'learning_rate': 4.928571428571429e-05, 'epoch': 1.64}\n",
      "{'loss': 3.4603, 'grad_norm': 1.277408480644226, 'learning_rate': 4.923809523809524e-05, 'epoch': 1.68}\n",
      "{'loss': 3.253, 'grad_norm': 1.2662519216537476, 'learning_rate': 4.919047619047619e-05, 'epoch': 1.73}\n",
      "{'loss': 3.2908, 'grad_norm': 1.3161895275115967, 'learning_rate': 4.9142857142857144e-05, 'epoch': 1.78}\n",
      "{'loss': 3.4049, 'grad_norm': 1.2200103998184204, 'learning_rate': 4.90952380952381e-05, 'epoch': 1.82}\n",
      "{'loss': 3.1701, 'grad_norm': 1.2020869255065918, 'learning_rate': 4.904761904761905e-05, 'epoch': 1.87}\n",
      "{'loss': 3.4067, 'grad_norm': 1.3063957691192627, 'learning_rate': 4.9e-05, 'epoch': 1.92}\n",
      "{'loss': 3.2646, 'grad_norm': 1.2154629230499268, 'learning_rate': 4.895238095238096e-05, 'epoch': 1.96}\n",
      "{'loss': 3.1875, 'grad_norm': 1.4205551147460938, 'learning_rate': 4.890476190476191e-05, 'epoch': 2.01}\n",
      "{'loss': 3.2474, 'grad_norm': 1.3451790809631348, 'learning_rate': 4.885714285714286e-05, 'epoch': 2.06}\n",
      "{'loss': 3.1892, 'grad_norm': 1.3681973218917847, 'learning_rate': 4.880952380952381e-05, 'epoch': 2.1}\n",
      "{'loss': 3.1917, 'grad_norm': 1.2537343502044678, 'learning_rate': 4.876190476190476e-05, 'epoch': 2.15}\n",
      "{'loss': 3.138, 'grad_norm': 1.3643630743026733, 'learning_rate': 4.8714285714285714e-05, 'epoch': 2.2}\n",
      "{'loss': 3.1453, 'grad_norm': 1.3366163969039917, 'learning_rate': 4.866666666666667e-05, 'epoch': 2.24}\n",
      "{'loss': 3.0778, 'grad_norm': 1.3137872219085693, 'learning_rate': 4.861904761904762e-05, 'epoch': 2.29}\n",
      "{'loss': 3.1069, 'grad_norm': 1.2469308376312256, 'learning_rate': 4.8571428571428576e-05, 'epoch': 2.34}\n",
      "{'loss': 3.1174, 'grad_norm': 1.3323464393615723, 'learning_rate': 4.852380952380953e-05, 'epoch': 2.38}\n",
      "{'loss': 3.1735, 'grad_norm': 1.3232707977294922, 'learning_rate': 4.847619047619048e-05, 'epoch': 2.43}\n",
      "{'loss': 3.1737, 'grad_norm': 1.2118455171585083, 'learning_rate': 4.842857142857143e-05, 'epoch': 2.48}\n",
      "{'loss': 3.0831, 'grad_norm': 1.2892613410949707, 'learning_rate': 4.838095238095238e-05, 'epoch': 2.52}\n",
      "{'loss': 3.0319, 'grad_norm': 1.4396365880966187, 'learning_rate': 4.8333333333333334e-05, 'epoch': 2.57}\n",
      "{'loss': 3.1442, 'grad_norm': 1.4776209592819214, 'learning_rate': 4.828571428571429e-05, 'epoch': 2.62}\n",
      "{'loss': 3.1931, 'grad_norm': 1.4798810482025146, 'learning_rate': 4.823809523809524e-05, 'epoch': 2.66}\n",
      "{'loss': 3.0684, 'grad_norm': 1.330257534980774, 'learning_rate': 4.819047619047619e-05, 'epoch': 2.71}\n",
      "{'loss': 3.1337, 'grad_norm': 1.2812215089797974, 'learning_rate': 4.8142857142857147e-05, 'epoch': 2.76}\n",
      "{'loss': 3.0662, 'grad_norm': 1.3618615865707397, 'learning_rate': 4.80952380952381e-05, 'epoch': 2.8}\n",
      "{'loss': 3.1633, 'grad_norm': 1.2817128896713257, 'learning_rate': 4.804761904761905e-05, 'epoch': 2.85}\n",
      "{'loss': 3.0308, 'grad_norm': 1.2548208236694336, 'learning_rate': 4.8e-05, 'epoch': 2.9}\n",
      "{'loss': 3.0297, 'grad_norm': 1.2670127153396606, 'learning_rate': 4.795238095238095e-05, 'epoch': 2.94}\n",
      "{'loss': 3.1047, 'grad_norm': 1.3067131042480469, 'learning_rate': 4.790476190476191e-05, 'epoch': 2.99}\n",
      "{'loss': 2.9512, 'grad_norm': 1.2575082778930664, 'learning_rate': 4.785714285714286e-05, 'epoch': 3.04}\n",
      "{'loss': 2.8712, 'grad_norm': 1.216971755027771, 'learning_rate': 4.780952380952381e-05, 'epoch': 3.08}\n",
      "{'loss': 2.9523, 'grad_norm': 1.2947089672088623, 'learning_rate': 4.7761904761904766e-05, 'epoch': 3.13}\n",
      "{'loss': 2.9552, 'grad_norm': 1.23309326171875, 'learning_rate': 4.771428571428572e-05, 'epoch': 3.18}\n",
      "{'loss': 3.0259, 'grad_norm': 1.4091768264770508, 'learning_rate': 4.766666666666667e-05, 'epoch': 3.22}\n",
      "{'loss': 2.9785, 'grad_norm': 1.2684391736984253, 'learning_rate': 4.761904761904762e-05, 'epoch': 3.27}\n",
      "{'loss': 2.9454, 'grad_norm': 3.5348451137542725, 'learning_rate': 4.757142857142857e-05, 'epoch': 3.32}\n",
      "{'loss': 2.9866, 'grad_norm': 1.182002305984497, 'learning_rate': 4.7523809523809523e-05, 'epoch': 3.36}\n",
      "{'loss': 2.9904, 'grad_norm': 1.2939283847808838, 'learning_rate': 4.747619047619048e-05, 'epoch': 3.41}\n",
      "{'loss': 3.0253, 'grad_norm': 1.2547669410705566, 'learning_rate': 4.742857142857143e-05, 'epoch': 3.46}\n",
      "{'loss': 2.8927, 'grad_norm': 1.2225899696350098, 'learning_rate': 4.738095238095238e-05, 'epoch': 3.5}\n",
      "{'loss': 2.8656, 'grad_norm': 1.2788918018341064, 'learning_rate': 4.7333333333333336e-05, 'epoch': 3.55}\n",
      "{'loss': 2.8876, 'grad_norm': 1.263242244720459, 'learning_rate': 4.728571428571429e-05, 'epoch': 3.6}\n",
      "{'loss': 2.9923, 'grad_norm': 1.3067070245742798, 'learning_rate': 4.723809523809524e-05, 'epoch': 3.64}\n",
      "{'loss': 2.9514, 'grad_norm': 1.2182140350341797, 'learning_rate': 4.719047619047619e-05, 'epoch': 3.69}\n",
      "{'loss': 3.0356, 'grad_norm': 1.3407588005065918, 'learning_rate': 4.714285714285714e-05, 'epoch': 3.74}\n",
      "{'loss': 2.8893, 'grad_norm': 1.2577005624771118, 'learning_rate': 4.70952380952381e-05, 'epoch': 3.79}\n",
      "{'loss': 2.9005, 'grad_norm': 1.2780225276947021, 'learning_rate': 4.704761904761905e-05, 'epoch': 3.83}\n",
      "{'loss': 2.8584, 'grad_norm': 1.2253035306930542, 'learning_rate': 4.7e-05, 'epoch': 3.88}\n",
      "{'loss': 2.8893, 'grad_norm': 1.2282932996749878, 'learning_rate': 4.6952380952380956e-05, 'epoch': 3.93}\n",
      "{'loss': 2.902, 'grad_norm': 1.2729970216751099, 'learning_rate': 4.690476190476191e-05, 'epoch': 3.97}\n",
      "{'loss': 2.8157, 'grad_norm': 1.3606257438659668, 'learning_rate': 4.685714285714286e-05, 'epoch': 4.02}\n",
      "{'loss': 2.9004, 'grad_norm': 1.3173444271087646, 'learning_rate': 4.680952380952382e-05, 'epoch': 4.07}\n",
      "{'loss': 2.7866, 'grad_norm': 1.3732786178588867, 'learning_rate': 4.676190476190476e-05, 'epoch': 4.11}\n",
      "{'loss': 2.7943, 'grad_norm': 1.2416019439697266, 'learning_rate': 4.671428571428571e-05, 'epoch': 4.16}\n",
      "{'loss': 2.7045, 'grad_norm': 1.3086588382720947, 'learning_rate': 4.666666666666667e-05, 'epoch': 4.21}\n",
      "{'loss': 2.8826, 'grad_norm': 1.2307016849517822, 'learning_rate': 4.661904761904762e-05, 'epoch': 4.25}\n",
      "{'loss': 2.5939, 'grad_norm': 1.2292789220809937, 'learning_rate': 4.6571428571428575e-05, 'epoch': 4.3}\n",
      "{'loss': 2.9364, 'grad_norm': 1.3122867345809937, 'learning_rate': 4.6523809523809526e-05, 'epoch': 4.35}\n",
      "{'loss': 2.8695, 'grad_norm': 1.2218897342681885, 'learning_rate': 4.647619047619048e-05, 'epoch': 4.39}\n",
      "{'loss': 2.8269, 'grad_norm': 1.2380722761154175, 'learning_rate': 4.642857142857143e-05, 'epoch': 4.44}\n",
      "{'loss': 2.7348, 'grad_norm': 1.2155247926712036, 'learning_rate': 4.638095238095238e-05, 'epoch': 4.49}\n",
      "{'loss': 2.8522, 'grad_norm': 1.258436679840088, 'learning_rate': 4.633333333333333e-05, 'epoch': 4.53}\n",
      "{'loss': 2.8696, 'grad_norm': 1.2900454998016357, 'learning_rate': 4.628571428571429e-05, 'epoch': 4.58}\n",
      "{'loss': 2.8149, 'grad_norm': 1.3240256309509277, 'learning_rate': 4.623809523809524e-05, 'epoch': 4.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8496, 'grad_norm': 1.2537803649902344, 'learning_rate': 4.6190476190476194e-05, 'epoch': 4.67}\n",
      "{'loss': 2.7426, 'grad_norm': 1.3840250968933105, 'learning_rate': 4.6142857142857145e-05, 'epoch': 4.72}\n",
      "{'loss': 2.8263, 'grad_norm': 1.2715978622436523, 'learning_rate': 4.60952380952381e-05, 'epoch': 4.77}\n",
      "{'loss': 2.85, 'grad_norm': 1.238516092300415, 'learning_rate': 4.604761904761905e-05, 'epoch': 4.81}\n",
      "{'loss': 2.8637, 'grad_norm': 1.2540247440338135, 'learning_rate': 4.600000000000001e-05, 'epoch': 4.86}\n",
      "{'loss': 2.6353, 'grad_norm': 1.2164103984832764, 'learning_rate': 4.595238095238095e-05, 'epoch': 4.91}\n",
      "{'loss': 2.7926, 'grad_norm': 1.2646557092666626, 'learning_rate': 4.59047619047619e-05, 'epoch': 4.95}\n",
      "{'loss': 2.8306, 'grad_norm': 2.4137585163116455, 'learning_rate': 4.585714285714286e-05, 'epoch': 5.0}\n",
      "{'loss': 2.7022, 'grad_norm': 1.258054256439209, 'learning_rate': 4.580952380952381e-05, 'epoch': 5.05}\n",
      "{'loss': 2.6308, 'grad_norm': 1.2519793510437012, 'learning_rate': 4.5761904761904765e-05, 'epoch': 5.09}\n",
      "{'loss': 2.7463, 'grad_norm': 1.3309921026229858, 'learning_rate': 4.5714285714285716e-05, 'epoch': 5.14}\n",
      "{'loss': 2.7366, 'grad_norm': 1.2557933330535889, 'learning_rate': 4.566666666666667e-05, 'epoch': 5.19}\n",
      "{'loss': 2.7678, 'grad_norm': 1.2948719263076782, 'learning_rate': 4.561904761904762e-05, 'epoch': 5.23}\n",
      "{'loss': 2.6779, 'grad_norm': 1.1898640394210815, 'learning_rate': 4.557142857142858e-05, 'epoch': 5.28}\n",
      "{'loss': 2.7775, 'grad_norm': 1.2524399757385254, 'learning_rate': 4.552380952380952e-05, 'epoch': 5.33}\n",
      "{'loss': 2.6555, 'grad_norm': 1.2599951028823853, 'learning_rate': 4.547619047619048e-05, 'epoch': 5.37}\n",
      "{'loss': 2.5721, 'grad_norm': 1.2256356477737427, 'learning_rate': 4.542857142857143e-05, 'epoch': 5.42}\n",
      "{'loss': 2.719, 'grad_norm': 1.3549015522003174, 'learning_rate': 4.5380952380952384e-05, 'epoch': 5.47}\n",
      "{'loss': 2.7034, 'grad_norm': 1.3601677417755127, 'learning_rate': 4.5333333333333335e-05, 'epoch': 5.51}\n",
      "{'loss': 2.6841, 'grad_norm': 1.3270552158355713, 'learning_rate': 4.528571428571429e-05, 'epoch': 5.56}\n",
      "{'loss': 2.6273, 'grad_norm': 1.309540033340454, 'learning_rate': 4.523809523809524e-05, 'epoch': 5.61}\n",
      "{'loss': 2.7259, 'grad_norm': 1.285936713218689, 'learning_rate': 4.51904761904762e-05, 'epoch': 5.65}\n",
      "{'loss': 2.6479, 'grad_norm': 1.309106469154358, 'learning_rate': 4.514285714285714e-05, 'epoch': 5.7}\n",
      "{'loss': 2.7346, 'grad_norm': 1.2900193929672241, 'learning_rate': 4.509523809523809e-05, 'epoch': 5.75}\n",
      "{'loss': 2.586, 'grad_norm': 1.2417906522750854, 'learning_rate': 4.504761904761905e-05, 'epoch': 5.79}\n",
      "{'loss': 2.6193, 'grad_norm': 1.2693010568618774, 'learning_rate': 4.5e-05, 'epoch': 5.84}\n",
      "{'loss': 2.717, 'grad_norm': 1.2985477447509766, 'learning_rate': 4.4952380952380954e-05, 'epoch': 5.89}\n",
      "{'loss': 2.6653, 'grad_norm': 1.2206928730010986, 'learning_rate': 4.4904761904761906e-05, 'epoch': 5.93}\n",
      "{'loss': 2.7748, 'grad_norm': 1.3245540857315063, 'learning_rate': 4.485714285714286e-05, 'epoch': 5.98}\n",
      "{'loss': 2.6025, 'grad_norm': 1.3578845262527466, 'learning_rate': 4.480952380952381e-05, 'epoch': 6.03}\n",
      "{'loss': 2.6403, 'grad_norm': 1.265307068824768, 'learning_rate': 4.476190476190477e-05, 'epoch': 6.07}\n",
      "{'loss': 2.5604, 'grad_norm': 1.3145321607589722, 'learning_rate': 4.471428571428571e-05, 'epoch': 6.12}\n",
      "{'loss': 2.6515, 'grad_norm': 1.2410368919372559, 'learning_rate': 4.466666666666667e-05, 'epoch': 6.17}\n",
      "{'loss': 2.4231, 'grad_norm': 1.2976679801940918, 'learning_rate': 4.461904761904762e-05, 'epoch': 6.21}\n",
      "{'loss': 2.5039, 'grad_norm': 1.2629425525665283, 'learning_rate': 4.4571428571428574e-05, 'epoch': 6.26}\n",
      "{'loss': 2.6358, 'grad_norm': 1.2880642414093018, 'learning_rate': 4.4523809523809525e-05, 'epoch': 6.31}\n",
      "{'loss': 2.5907, 'grad_norm': 1.3125107288360596, 'learning_rate': 4.447619047619048e-05, 'epoch': 6.36}\n",
      "{'loss': 2.592, 'grad_norm': 1.2835099697113037, 'learning_rate': 4.442857142857143e-05, 'epoch': 6.4}\n",
      "{'loss': 2.6984, 'grad_norm': 1.3774930238723755, 'learning_rate': 4.4380952380952386e-05, 'epoch': 6.45}\n",
      "{'loss': 2.5382, 'grad_norm': 1.255630612373352, 'learning_rate': 4.433333333333334e-05, 'epoch': 6.5}\n",
      "{'loss': 2.6309, 'grad_norm': 1.2381480932235718, 'learning_rate': 4.428571428571428e-05, 'epoch': 6.54}\n",
      "{'loss': 2.6083, 'grad_norm': 1.2660328149795532, 'learning_rate': 4.423809523809524e-05, 'epoch': 6.59}\n",
      "{'loss': 2.5841, 'grad_norm': 1.2769771814346313, 'learning_rate': 4.419047619047619e-05, 'epoch': 6.64}\n",
      "{'loss': 2.5468, 'grad_norm': 1.3670998811721802, 'learning_rate': 4.4142857142857144e-05, 'epoch': 6.68}\n",
      "{'loss': 2.6243, 'grad_norm': 1.3779820203781128, 'learning_rate': 4.4095238095238096e-05, 'epoch': 6.73}\n",
      "{'loss': 2.5033, 'grad_norm': 1.2954970598220825, 'learning_rate': 4.404761904761905e-05, 'epoch': 6.78}\n",
      "{'loss': 2.5327, 'grad_norm': 1.3188687562942505, 'learning_rate': 4.4000000000000006e-05, 'epoch': 6.82}\n",
      "{'loss': 2.6432, 'grad_norm': 1.3505644798278809, 'learning_rate': 4.395238095238096e-05, 'epoch': 6.87}\n",
      "{'loss': 2.4957, 'grad_norm': 1.2731672525405884, 'learning_rate': 4.39047619047619e-05, 'epoch': 6.92}\n",
      "{'loss': 2.7054, 'grad_norm': 1.2240864038467407, 'learning_rate': 4.385714285714286e-05, 'epoch': 6.96}\n",
      "{'loss': 2.5581, 'grad_norm': 1.3086820840835571, 'learning_rate': 4.380952380952381e-05, 'epoch': 7.01}\n",
      "{'loss': 2.5476, 'grad_norm': 1.3189525604248047, 'learning_rate': 4.376190476190476e-05, 'epoch': 7.06}\n",
      "{'loss': 2.5012, 'grad_norm': 1.275978922843933, 'learning_rate': 4.371428571428572e-05, 'epoch': 7.1}\n",
      "{'loss': 2.558, 'grad_norm': 1.2923972606658936, 'learning_rate': 4.3666666666666666e-05, 'epoch': 7.15}\n",
      "{'loss': 2.4999, 'grad_norm': 1.2843973636627197, 'learning_rate': 4.361904761904762e-05, 'epoch': 7.2}\n",
      "{'loss': 2.5706, 'grad_norm': 1.398419737815857, 'learning_rate': 4.3571428571428576e-05, 'epoch': 7.24}\n",
      "{'loss': 2.4967, 'grad_norm': 4.968869209289551, 'learning_rate': 4.352380952380953e-05, 'epoch': 7.29}\n",
      "{'loss': 2.5441, 'grad_norm': 1.3200840950012207, 'learning_rate': 4.347619047619047e-05, 'epoch': 7.34}\n",
      "{'loss': 2.5616, 'grad_norm': 1.28398597240448, 'learning_rate': 4.342857142857143e-05, 'epoch': 7.38}\n",
      "{'loss': 2.46, 'grad_norm': 1.2722896337509155, 'learning_rate': 4.338095238095238e-05, 'epoch': 7.43}\n",
      "{'loss': 2.3651, 'grad_norm': 1.2848484516143799, 'learning_rate': 4.3333333333333334e-05, 'epoch': 7.48}\n",
      "{'loss': 2.4003, 'grad_norm': 1.2210654020309448, 'learning_rate': 4.328571428571429e-05, 'epoch': 7.52}\n",
      "{'loss': 2.4189, 'grad_norm': 1.3388899564743042, 'learning_rate': 4.323809523809524e-05, 'epoch': 7.57}\n",
      "{'loss': 2.5269, 'grad_norm': 1.3143525123596191, 'learning_rate': 4.3190476190476195e-05, 'epoch': 7.62}\n",
      "{'loss': 2.6358, 'grad_norm': 1.4175140857696533, 'learning_rate': 4.314285714285715e-05, 'epoch': 7.66}\n",
      "{'loss': 2.4274, 'grad_norm': 1.23970627784729, 'learning_rate': 4.30952380952381e-05, 'epoch': 7.71}\n",
      "{'loss': 2.4244, 'grad_norm': 1.3811346292495728, 'learning_rate': 4.304761904761905e-05, 'epoch': 7.76}\n",
      "{'loss': 2.4265, 'grad_norm': 1.3000210523605347, 'learning_rate': 4.3e-05, 'epoch': 7.8}\n",
      "{'loss': 2.5302, 'grad_norm': 1.341688871383667, 'learning_rate': 4.295238095238095e-05, 'epoch': 7.85}\n",
      "{'loss': 2.6366, 'grad_norm': 1.3591572046279907, 'learning_rate': 4.290476190476191e-05, 'epoch': 7.9}\n",
      "{'loss': 2.3657, 'grad_norm': 1.280948281288147, 'learning_rate': 4.2857142857142856e-05, 'epoch': 7.94}\n",
      "{'loss': 2.4177, 'grad_norm': 1.3368414640426636, 'learning_rate': 4.280952380952381e-05, 'epoch': 7.99}\n",
      "{'loss': 2.4791, 'grad_norm': 1.3050721883773804, 'learning_rate': 4.2761904761904766e-05, 'epoch': 8.04}\n",
      "{'loss': 2.3885, 'grad_norm': 1.3594855070114136, 'learning_rate': 4.271428571428572e-05, 'epoch': 8.08}\n",
      "{'loss': 2.3291, 'grad_norm': 1.311699628829956, 'learning_rate': 4.266666666666667e-05, 'epoch': 8.13}\n",
      "{'loss': 2.4507, 'grad_norm': 1.315441608428955, 'learning_rate': 4.261904761904762e-05, 'epoch': 8.18}\n",
      "{'loss': 2.4589, 'grad_norm': 1.2999024391174316, 'learning_rate': 4.257142857142857e-05, 'epoch': 8.22}\n",
      "{'loss': 2.4008, 'grad_norm': 1.2530910968780518, 'learning_rate': 4.2523809523809524e-05, 'epoch': 8.27}\n",
      "{'loss': 2.4334, 'grad_norm': 1.3187333345413208, 'learning_rate': 4.247619047619048e-05, 'epoch': 8.32}\n",
      "{'loss': 2.2801, 'grad_norm': 1.3755714893341064, 'learning_rate': 4.242857142857143e-05, 'epoch': 8.36}\n",
      "{'loss': 2.4465, 'grad_norm': 1.4043922424316406, 'learning_rate': 4.2380952380952385e-05, 'epoch': 8.41}\n",
      "{'loss': 2.3763, 'grad_norm': 1.3058700561523438, 'learning_rate': 4.233333333333334e-05, 'epoch': 8.46}\n",
      "{'loss': 2.4769, 'grad_norm': 1.326600193977356, 'learning_rate': 4.228571428571429e-05, 'epoch': 8.5}\n",
      "{'loss': 2.4586, 'grad_norm': 1.4111007452011108, 'learning_rate': 4.223809523809524e-05, 'epoch': 8.55}\n",
      "{'loss': 2.3919, 'grad_norm': 1.3696779012680054, 'learning_rate': 4.219047619047619e-05, 'epoch': 8.6}\n",
      "{'loss': 2.3729, 'grad_norm': 1.3804668188095093, 'learning_rate': 4.214285714285714e-05, 'epoch': 8.64}\n",
      "{'loss': 2.4157, 'grad_norm': 1.2848358154296875, 'learning_rate': 4.20952380952381e-05, 'epoch': 8.69}\n",
      "{'loss': 2.3189, 'grad_norm': 1.4236708879470825, 'learning_rate': 4.204761904761905e-05, 'epoch': 8.74}\n",
      "{'loss': 2.3563, 'grad_norm': 1.3683708906173706, 'learning_rate': 4.2e-05, 'epoch': 8.79}\n",
      "{'loss': 2.3797, 'grad_norm': 1.2833877801895142, 'learning_rate': 4.1952380952380956e-05, 'epoch': 8.83}\n",
      "{'loss': 2.3989, 'grad_norm': 1.436147689819336, 'learning_rate': 4.190476190476191e-05, 'epoch': 8.88}\n",
      "{'loss': 2.353, 'grad_norm': 1.335687518119812, 'learning_rate': 4.185714285714286e-05, 'epoch': 8.93}\n",
      "{'loss': 2.3908, 'grad_norm': 1.3481590747833252, 'learning_rate': 4.180952380952381e-05, 'epoch': 8.97}\n",
      "{'loss': 2.5373, 'grad_norm': 1.3081108331680298, 'learning_rate': 4.176190476190476e-05, 'epoch': 9.02}\n",
      "{'loss': 2.3191, 'grad_norm': 1.3127015829086304, 'learning_rate': 4.1714285714285714e-05, 'epoch': 9.07}\n",
      "{'loss': 2.2571, 'grad_norm': 1.4128360748291016, 'learning_rate': 4.166666666666667e-05, 'epoch': 9.11}\n",
      "{'loss': 2.4248, 'grad_norm': 1.6310051679611206, 'learning_rate': 4.161904761904762e-05, 'epoch': 9.16}\n",
      "{'loss': 2.2182, 'grad_norm': 1.3410056829452515, 'learning_rate': 4.1571428571428575e-05, 'epoch': 9.21}\n",
      "{'loss': 2.2878, 'grad_norm': 1.38614022731781, 'learning_rate': 4.152380952380953e-05, 'epoch': 9.25}\n",
      "{'loss': 2.3166, 'grad_norm': 1.372084140777588, 'learning_rate': 4.147619047619048e-05, 'epoch': 9.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3176, 'grad_norm': 1.429556131362915, 'learning_rate': 4.1428571428571437e-05, 'epoch': 9.35}\n",
      "{'loss': 2.3186, 'grad_norm': 1.3171312808990479, 'learning_rate': 4.138095238095238e-05, 'epoch': 9.39}\n",
      "{'loss': 2.3181, 'grad_norm': 1.3778603076934814, 'learning_rate': 4.133333333333333e-05, 'epoch': 9.44}\n",
      "{'loss': 2.3488, 'grad_norm': 1.3214688301086426, 'learning_rate': 4.128571428571429e-05, 'epoch': 9.49}\n",
      "{'loss': 2.3836, 'grad_norm': 1.3523541688919067, 'learning_rate': 4.123809523809524e-05, 'epoch': 9.53}\n",
      "{'loss': 2.2565, 'grad_norm': 1.3500161170959473, 'learning_rate': 4.119047619047619e-05, 'epoch': 9.58}\n",
      "{'loss': 2.3662, 'grad_norm': 1.3217079639434814, 'learning_rate': 4.1142857142857146e-05, 'epoch': 9.63}\n",
      "{'loss': 2.2233, 'grad_norm': 1.317834734916687, 'learning_rate': 4.10952380952381e-05, 'epoch': 9.67}\n",
      "{'loss': 2.2936, 'grad_norm': 1.4396942853927612, 'learning_rate': 4.104761904761905e-05, 'epoch': 9.72}\n",
      "{'loss': 2.3458, 'grad_norm': 1.3179991245269775, 'learning_rate': 4.1e-05, 'epoch': 9.77}\n",
      "{'loss': 2.2919, 'grad_norm': 1.3378627300262451, 'learning_rate': 4.095238095238095e-05, 'epoch': 9.81}\n",
      "{'loss': 2.4636, 'grad_norm': 1.362439513206482, 'learning_rate': 4.0904761904761904e-05, 'epoch': 9.86}\n",
      "{'loss': 2.2236, 'grad_norm': 1.324846863746643, 'learning_rate': 4.085714285714286e-05, 'epoch': 9.91}\n",
      "{'loss': 2.2612, 'grad_norm': 1.3196778297424316, 'learning_rate': 4.0809523809523813e-05, 'epoch': 9.95}\n",
      "{'loss': 2.3778, 'grad_norm': 2.5952892303466797, 'learning_rate': 4.0761904761904765e-05, 'epoch': 10.0}\n",
      "{'loss': 2.2234, 'grad_norm': 1.4576878547668457, 'learning_rate': 4.0714285714285717e-05, 'epoch': 10.05}\n",
      "{'loss': 2.3087, 'grad_norm': 1.3530617952346802, 'learning_rate': 4.066666666666667e-05, 'epoch': 10.09}\n",
      "{'loss': 2.1945, 'grad_norm': 1.4287304878234863, 'learning_rate': 4.0619047619047626e-05, 'epoch': 10.14}\n",
      "{'loss': 2.2303, 'grad_norm': 1.3514606952667236, 'learning_rate': 4.057142857142857e-05, 'epoch': 10.19}\n",
      "{'loss': 2.2492, 'grad_norm': 1.4480607509613037, 'learning_rate': 4.052380952380952e-05, 'epoch': 10.23}\n",
      "{'loss': 2.1611, 'grad_norm': 1.334425926208496, 'learning_rate': 4.047619047619048e-05, 'epoch': 10.28}\n",
      "{'loss': 2.1571, 'grad_norm': 1.2852708101272583, 'learning_rate': 4.042857142857143e-05, 'epoch': 10.33}\n",
      "{'loss': 2.2793, 'grad_norm': 1.419207215309143, 'learning_rate': 4.038095238095238e-05, 'epoch': 10.37}\n",
      "{'loss': 2.2124, 'grad_norm': 1.4189950227737427, 'learning_rate': 4.0333333333333336e-05, 'epoch': 10.42}\n",
      "{'loss': 2.296, 'grad_norm': 1.4454383850097656, 'learning_rate': 4.028571428571429e-05, 'epoch': 10.47}\n",
      "{'loss': 2.2588, 'grad_norm': 1.310543417930603, 'learning_rate': 4.023809523809524e-05, 'epoch': 10.51}\n",
      "{'loss': 2.2799, 'grad_norm': 1.4100656509399414, 'learning_rate': 4.01904761904762e-05, 'epoch': 10.56}\n",
      "{'loss': 2.2515, 'grad_norm': 1.3547401428222656, 'learning_rate': 4.014285714285714e-05, 'epoch': 10.61}\n",
      "{'loss': 2.2227, 'grad_norm': 1.3468459844589233, 'learning_rate': 4.00952380952381e-05, 'epoch': 10.65}\n",
      "{'loss': 2.2303, 'grad_norm': 1.3326741456985474, 'learning_rate': 4.004761904761905e-05, 'epoch': 10.7}\n",
      "{'loss': 2.3364, 'grad_norm': 1.4446263313293457, 'learning_rate': 4e-05, 'epoch': 10.75}\n",
      "{'loss': 2.2208, 'grad_norm': 1.4061243534088135, 'learning_rate': 3.9952380952380955e-05, 'epoch': 10.79}\n",
      "{'loss': 2.139, 'grad_norm': 1.3897114992141724, 'learning_rate': 3.9904761904761906e-05, 'epoch': 10.84}\n",
      "{'loss': 2.2208, 'grad_norm': 1.246246337890625, 'learning_rate': 3.985714285714286e-05, 'epoch': 10.89}\n",
      "{'loss': 2.1711, 'grad_norm': 1.4445637464523315, 'learning_rate': 3.9809523809523816e-05, 'epoch': 10.93}\n",
      "{'loss': 2.2757, 'grad_norm': 1.4480525255203247, 'learning_rate': 3.976190476190476e-05, 'epoch': 10.98}\n",
      "{'loss': 2.2215, 'grad_norm': 1.3836411237716675, 'learning_rate': 3.971428571428571e-05, 'epoch': 11.03}\n",
      "{'loss': 2.1015, 'grad_norm': 1.385431170463562, 'learning_rate': 3.966666666666667e-05, 'epoch': 11.07}\n",
      "{'loss': 2.2264, 'grad_norm': 1.5336593389511108, 'learning_rate': 3.961904761904762e-05, 'epoch': 11.12}\n",
      "{'loss': 2.2485, 'grad_norm': 1.3911467790603638, 'learning_rate': 3.9571428571428574e-05, 'epoch': 11.17}\n",
      "{'loss': 2.1828, 'grad_norm': 1.4253642559051514, 'learning_rate': 3.9523809523809526e-05, 'epoch': 11.21}\n",
      "{'loss': 2.1241, 'grad_norm': 1.44778311252594, 'learning_rate': 3.947619047619048e-05, 'epoch': 11.26}\n",
      "{'loss': 2.0888, 'grad_norm': 1.6193077564239502, 'learning_rate': 3.942857142857143e-05, 'epoch': 11.31}\n",
      "{'loss': 2.18, 'grad_norm': 1.4451112747192383, 'learning_rate': 3.938095238095239e-05, 'epoch': 11.36}\n",
      "{'loss': 2.1711, 'grad_norm': 1.406646490097046, 'learning_rate': 3.933333333333333e-05, 'epoch': 11.4}\n",
      "{'loss': 2.036, 'grad_norm': 1.3552958965301514, 'learning_rate': 3.928571428571429e-05, 'epoch': 11.45}\n",
      "{'loss': 2.1418, 'grad_norm': 1.4041595458984375, 'learning_rate': 3.923809523809524e-05, 'epoch': 11.5}\n",
      "{'loss': 2.1025, 'grad_norm': 1.4029837846755981, 'learning_rate': 3.919047619047619e-05, 'epoch': 11.54}\n",
      "{'loss': 2.1208, 'grad_norm': 4.132309436798096, 'learning_rate': 3.9142857142857145e-05, 'epoch': 11.59}\n",
      "{'loss': 2.1792, 'grad_norm': 1.4235814809799194, 'learning_rate': 3.9095238095238096e-05, 'epoch': 11.64}\n",
      "{'loss': 2.1503, 'grad_norm': 1.5109567642211914, 'learning_rate': 3.904761904761905e-05, 'epoch': 11.68}\n",
      "{'loss': 2.2047, 'grad_norm': 1.4255105257034302, 'learning_rate': 3.9000000000000006e-05, 'epoch': 11.73}\n",
      "{'loss': 2.2185, 'grad_norm': 1.4297500848770142, 'learning_rate': 3.895238095238096e-05, 'epoch': 11.78}\n",
      "{'loss': 2.1611, 'grad_norm': 1.424347162246704, 'learning_rate': 3.89047619047619e-05, 'epoch': 11.82}\n",
      "{'loss': 2.1932, 'grad_norm': 1.4205516576766968, 'learning_rate': 3.885714285714286e-05, 'epoch': 11.87}\n",
      "{'loss': 2.16, 'grad_norm': 1.4223357439041138, 'learning_rate': 3.880952380952381e-05, 'epoch': 11.92}\n",
      "{'loss': 2.08, 'grad_norm': 1.407141923904419, 'learning_rate': 3.8761904761904764e-05, 'epoch': 11.96}\n",
      "{'loss': 2.1891, 'grad_norm': 1.3814359903335571, 'learning_rate': 3.8714285714285715e-05, 'epoch': 12.01}\n",
      "{'loss': 2.0355, 'grad_norm': 1.4501503705978394, 'learning_rate': 3.866666666666667e-05, 'epoch': 12.06}\n",
      "{'loss': 2.0244, 'grad_norm': 1.4996806383132935, 'learning_rate': 3.861904761904762e-05, 'epoch': 12.1}\n",
      "{'loss': 2.0841, 'grad_norm': 1.407758355140686, 'learning_rate': 3.857142857142858e-05, 'epoch': 12.15}\n",
      "{'loss': 2.0974, 'grad_norm': 1.4671199321746826, 'learning_rate': 3.852380952380952e-05, 'epoch': 12.2}\n",
      "{'loss': 2.0477, 'grad_norm': 1.351806402206421, 'learning_rate': 3.847619047619048e-05, 'epoch': 12.24}\n",
      "{'loss': 2.0935, 'grad_norm': 1.4527868032455444, 'learning_rate': 3.842857142857143e-05, 'epoch': 12.29}\n",
      "{'loss': 2.1576, 'grad_norm': 1.379296064376831, 'learning_rate': 3.838095238095238e-05, 'epoch': 12.34}\n",
      "{'loss': 2.1486, 'grad_norm': 1.4670264720916748, 'learning_rate': 3.8333333333333334e-05, 'epoch': 12.38}\n",
      "{'loss': 2.0672, 'grad_norm': 1.4365049600601196, 'learning_rate': 3.8285714285714286e-05, 'epoch': 12.43}\n",
      "{'loss': 2.0452, 'grad_norm': 1.4639849662780762, 'learning_rate': 3.823809523809524e-05, 'epoch': 12.48}\n",
      "{'loss': 2.0459, 'grad_norm': 1.5012153387069702, 'learning_rate': 3.8190476190476196e-05, 'epoch': 12.52}\n",
      "{'loss': 2.1779, 'grad_norm': 1.4233267307281494, 'learning_rate': 3.814285714285715e-05, 'epoch': 12.57}\n",
      "{'loss': 2.0671, 'grad_norm': 1.4124282598495483, 'learning_rate': 3.809523809523809e-05, 'epoch': 12.62}\n",
      "{'loss': 2.0905, 'grad_norm': 1.428707242012024, 'learning_rate': 3.804761904761905e-05, 'epoch': 12.66}\n",
      "{'loss': 2.078, 'grad_norm': 1.4253758192062378, 'learning_rate': 3.8e-05, 'epoch': 12.71}\n",
      "{'loss': 2.1248, 'grad_norm': 1.5349735021591187, 'learning_rate': 3.7952380952380954e-05, 'epoch': 12.76}\n",
      "{'loss': 2.1496, 'grad_norm': 1.3868058919906616, 'learning_rate': 3.7904761904761905e-05, 'epoch': 12.8}\n",
      "{'loss': 2.0431, 'grad_norm': 1.4000221490859985, 'learning_rate': 3.785714285714286e-05, 'epoch': 12.85}\n",
      "{'loss': 2.0115, 'grad_norm': 1.498466968536377, 'learning_rate': 3.780952380952381e-05, 'epoch': 12.9}\n",
      "{'loss': 2.0965, 'grad_norm': 1.4811253547668457, 'learning_rate': 3.7761904761904767e-05, 'epoch': 12.94}\n",
      "{'loss': 2.0823, 'grad_norm': 1.4836642742156982, 'learning_rate': 3.771428571428572e-05, 'epoch': 12.99}\n",
      "{'loss': 1.9976, 'grad_norm': 1.4239956140518188, 'learning_rate': 3.766666666666667e-05, 'epoch': 13.04}\n",
      "{'loss': 2.0608, 'grad_norm': 1.405575156211853, 'learning_rate': 3.761904761904762e-05, 'epoch': 13.08}\n",
      "{'loss': 1.9733, 'grad_norm': 1.3118901252746582, 'learning_rate': 3.757142857142857e-05, 'epoch': 13.13}\n",
      "{'loss': 1.9762, 'grad_norm': 1.4354722499847412, 'learning_rate': 3.752380952380953e-05, 'epoch': 13.18}\n",
      "{'loss': 2.0815, 'grad_norm': 1.4852874279022217, 'learning_rate': 3.7476190476190476e-05, 'epoch': 13.22}\n",
      "{'loss': 2.0134, 'grad_norm': 1.4883909225463867, 'learning_rate': 3.742857142857143e-05, 'epoch': 13.27}\n",
      "{'loss': 2.055, 'grad_norm': 1.5387322902679443, 'learning_rate': 3.7380952380952386e-05, 'epoch': 13.32}\n",
      "{'loss': 2.0051, 'grad_norm': 1.5595927238464355, 'learning_rate': 3.733333333333334e-05, 'epoch': 13.36}\n",
      "{'loss': 1.915, 'grad_norm': 1.3691964149475098, 'learning_rate': 3.728571428571428e-05, 'epoch': 13.41}\n",
      "{'loss': 2.0278, 'grad_norm': 1.5392242670059204, 'learning_rate': 3.723809523809524e-05, 'epoch': 13.46}\n",
      "{'loss': 1.9446, 'grad_norm': 1.4516706466674805, 'learning_rate': 3.719047619047619e-05, 'epoch': 13.5}\n",
      "{'loss': 2.0482, 'grad_norm': 1.4761680364608765, 'learning_rate': 3.7142857142857143e-05, 'epoch': 13.55}\n",
      "{'loss': 1.9033, 'grad_norm': 1.445383071899414, 'learning_rate': 3.70952380952381e-05, 'epoch': 13.6}\n",
      "{'loss': 1.9841, 'grad_norm': 1.4433250427246094, 'learning_rate': 3.7047619047619047e-05, 'epoch': 13.64}\n",
      "{'loss': 2.0987, 'grad_norm': 1.4974673986434937, 'learning_rate': 3.7e-05, 'epoch': 13.69}\n",
      "{'loss': 2.0652, 'grad_norm': 1.4763070344924927, 'learning_rate': 3.6952380952380956e-05, 'epoch': 13.74}\n",
      "{'loss': 1.9833, 'grad_norm': 1.4313745498657227, 'learning_rate': 3.690476190476191e-05, 'epoch': 13.79}\n",
      "{'loss': 2.0677, 'grad_norm': 1.5452508926391602, 'learning_rate': 3.685714285714286e-05, 'epoch': 13.83}\n",
      "{'loss': 2.0305, 'grad_norm': 1.4568735361099243, 'learning_rate': 3.680952380952381e-05, 'epoch': 13.88}\n",
      "{'loss': 1.9859, 'grad_norm': 1.5087810754776, 'learning_rate': 3.676190476190476e-05, 'epoch': 13.93}\n",
      "{'loss': 2.0904, 'grad_norm': 1.4481724500656128, 'learning_rate': 3.671428571428572e-05, 'epoch': 13.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0136, 'grad_norm': 1.3976092338562012, 'learning_rate': 3.6666666666666666e-05, 'epoch': 14.02}\n",
      "{'loss': 1.9248, 'grad_norm': 1.5213935375213623, 'learning_rate': 3.661904761904762e-05, 'epoch': 14.07}\n",
      "{'loss': 1.9763, 'grad_norm': 1.5974228382110596, 'learning_rate': 3.6571428571428576e-05, 'epoch': 14.11}\n",
      "{'loss': 1.9442, 'grad_norm': 1.4426779747009277, 'learning_rate': 3.652380952380953e-05, 'epoch': 14.16}\n",
      "{'loss': 1.912, 'grad_norm': 1.5180425643920898, 'learning_rate': 3.647619047619048e-05, 'epoch': 14.21}\n",
      "{'loss': 1.9717, 'grad_norm': 1.4170174598693848, 'learning_rate': 3.642857142857143e-05, 'epoch': 14.25}\n",
      "{'loss': 2.0068, 'grad_norm': 1.6478620767593384, 'learning_rate': 3.638095238095238e-05, 'epoch': 14.3}\n",
      "{'loss': 1.9178, 'grad_norm': 3.4148108959198, 'learning_rate': 3.633333333333333e-05, 'epoch': 14.35}\n",
      "{'loss': 1.8878, 'grad_norm': 2.3817217350006104, 'learning_rate': 3.628571428571429e-05, 'epoch': 14.39}\n",
      "{'loss': 1.9719, 'grad_norm': 1.3976795673370361, 'learning_rate': 3.6238095238095236e-05, 'epoch': 14.44}\n",
      "{'loss': 1.965, 'grad_norm': 1.5320537090301514, 'learning_rate': 3.619047619047619e-05, 'epoch': 14.49}\n",
      "{'loss': 2.0173, 'grad_norm': 1.4861762523651123, 'learning_rate': 3.6142857142857146e-05, 'epoch': 14.53}\n",
      "{'loss': 2.0024, 'grad_norm': 1.480898380279541, 'learning_rate': 3.60952380952381e-05, 'epoch': 14.58}\n",
      "{'loss': 1.9804, 'grad_norm': 1.4710263013839722, 'learning_rate': 3.604761904761905e-05, 'epoch': 14.63}\n",
      "{'loss': 1.9003, 'grad_norm': 1.491515040397644, 'learning_rate': 3.6e-05, 'epoch': 14.67}\n",
      "{'loss': 1.9451, 'grad_norm': 1.4338103532791138, 'learning_rate': 3.595238095238095e-05, 'epoch': 14.72}\n",
      "{'loss': 1.8952, 'grad_norm': 1.4805430173873901, 'learning_rate': 3.590476190476191e-05, 'epoch': 14.77}\n",
      "{'loss': 1.8503, 'grad_norm': 1.4686453342437744, 'learning_rate': 3.585714285714286e-05, 'epoch': 14.81}\n",
      "{'loss': 1.9825, 'grad_norm': 1.3938686847686768, 'learning_rate': 3.580952380952381e-05, 'epoch': 14.86}\n",
      "{'loss': 1.9783, 'grad_norm': 1.4843826293945312, 'learning_rate': 3.5761904761904765e-05, 'epoch': 14.91}\n",
      "{'loss': 1.9813, 'grad_norm': 1.5275238752365112, 'learning_rate': 3.571428571428572e-05, 'epoch': 14.95}\n",
      "{'loss': 1.9946, 'grad_norm': 3.0025417804718018, 'learning_rate': 3.566666666666667e-05, 'epoch': 15.0}\n",
      "{'loss': 1.8402, 'grad_norm': 1.4546605348587036, 'learning_rate': 3.561904761904762e-05, 'epoch': 15.05}\n",
      "{'loss': 1.8325, 'grad_norm': 1.5550135374069214, 'learning_rate': 3.557142857142857e-05, 'epoch': 15.09}\n",
      "{'loss': 1.8971, 'grad_norm': 1.552811622619629, 'learning_rate': 3.552380952380952e-05, 'epoch': 15.14}\n",
      "{'loss': 1.902, 'grad_norm': 1.5759252309799194, 'learning_rate': 3.547619047619048e-05, 'epoch': 15.19}\n",
      "{'loss': 1.8498, 'grad_norm': 1.453891396522522, 'learning_rate': 3.5428571428571426e-05, 'epoch': 15.23}\n",
      "{'loss': 1.8093, 'grad_norm': 1.4454026222229004, 'learning_rate': 3.5380952380952385e-05, 'epoch': 15.28}\n",
      "{'loss': 1.9428, 'grad_norm': 1.4391250610351562, 'learning_rate': 3.5333333333333336e-05, 'epoch': 15.33}\n",
      "{'loss': 1.9306, 'grad_norm': 1.4863734245300293, 'learning_rate': 3.528571428571429e-05, 'epoch': 15.37}\n",
      "{'loss': 1.8343, 'grad_norm': 1.461295247077942, 'learning_rate': 3.523809523809524e-05, 'epoch': 15.42}\n",
      "{'loss': 1.8308, 'grad_norm': 1.5212862491607666, 'learning_rate': 3.519047619047619e-05, 'epoch': 15.47}\n",
      "{'loss': 1.9708, 'grad_norm': 1.4706249237060547, 'learning_rate': 3.514285714285714e-05, 'epoch': 15.51}\n",
      "{'loss': 1.9548, 'grad_norm': 1.5459825992584229, 'learning_rate': 3.50952380952381e-05, 'epoch': 15.56}\n",
      "{'loss': 1.8452, 'grad_norm': 1.6201510429382324, 'learning_rate': 3.504761904761905e-05, 'epoch': 15.61}\n",
      "{'loss': 1.9278, 'grad_norm': 1.5480984449386597, 'learning_rate': 3.5e-05, 'epoch': 15.65}\n",
      "{'loss': 1.8166, 'grad_norm': 1.541969656944275, 'learning_rate': 3.4952380952380955e-05, 'epoch': 15.7}\n",
      "{'loss': 1.8706, 'grad_norm': 1.5065412521362305, 'learning_rate': 3.490476190476191e-05, 'epoch': 15.75}\n",
      "{'loss': 1.9221, 'grad_norm': 1.493141531944275, 'learning_rate': 3.485714285714286e-05, 'epoch': 15.79}\n",
      "{'loss': 1.9091, 'grad_norm': 1.5081087350845337, 'learning_rate': 3.480952380952381e-05, 'epoch': 15.84}\n",
      "{'loss': 1.8998, 'grad_norm': 1.5236917734146118, 'learning_rate': 3.476190476190476e-05, 'epoch': 15.89}\n",
      "{'loss': 1.9393, 'grad_norm': 1.5292906761169434, 'learning_rate': 3.471428571428571e-05, 'epoch': 15.93}\n",
      "{'loss': 1.9209, 'grad_norm': 1.560886025428772, 'learning_rate': 3.466666666666667e-05, 'epoch': 15.98}\n",
      "{'loss': 1.7613, 'grad_norm': 1.5184876918792725, 'learning_rate': 3.461904761904762e-05, 'epoch': 16.03}\n",
      "{'loss': 1.8185, 'grad_norm': 1.5945686101913452, 'learning_rate': 3.4571428571428574e-05, 'epoch': 16.07}\n",
      "{'loss': 1.8073, 'grad_norm': 1.4466397762298584, 'learning_rate': 3.4523809523809526e-05, 'epoch': 16.12}\n",
      "{'loss': 1.7973, 'grad_norm': 1.51095449924469, 'learning_rate': 3.447619047619048e-05, 'epoch': 16.17}\n",
      "{'loss': 1.7705, 'grad_norm': 1.5456980466842651, 'learning_rate': 3.442857142857143e-05, 'epoch': 16.21}\n",
      "{'loss': 1.8338, 'grad_norm': 1.5807511806488037, 'learning_rate': 3.438095238095238e-05, 'epoch': 16.26}\n",
      "{'loss': 1.8112, 'grad_norm': 1.6012943983078003, 'learning_rate': 3.433333333333333e-05, 'epoch': 16.31}\n",
      "{'loss': 1.761, 'grad_norm': 1.5134669542312622, 'learning_rate': 3.428571428571429e-05, 'epoch': 16.36}\n",
      "{'loss': 1.7627, 'grad_norm': 1.570567011833191, 'learning_rate': 3.423809523809524e-05, 'epoch': 16.4}\n",
      "{'loss': 1.8789, 'grad_norm': 1.6194578409194946, 'learning_rate': 3.419047619047619e-05, 'epoch': 16.45}\n",
      "{'loss': 1.8203, 'grad_norm': 1.6165093183517456, 'learning_rate': 3.4142857142857145e-05, 'epoch': 16.5}\n",
      "{'loss': 1.8285, 'grad_norm': 1.4797677993774414, 'learning_rate': 3.40952380952381e-05, 'epoch': 16.54}\n",
      "{'loss': 1.8373, 'grad_norm': 1.4796571731567383, 'learning_rate': 3.404761904761905e-05, 'epoch': 16.59}\n",
      "{'loss': 1.8432, 'grad_norm': 1.4153056144714355, 'learning_rate': 3.4000000000000007e-05, 'epoch': 16.64}\n",
      "{'loss': 1.8827, 'grad_norm': 1.5318608283996582, 'learning_rate': 3.395238095238095e-05, 'epoch': 16.68}\n",
      "{'loss': 1.8816, 'grad_norm': 1.5432136058807373, 'learning_rate': 3.39047619047619e-05, 'epoch': 16.73}\n",
      "{'loss': 1.8571, 'grad_norm': 1.5179524421691895, 'learning_rate': 3.385714285714286e-05, 'epoch': 16.78}\n",
      "{'loss': 1.821, 'grad_norm': 1.4558885097503662, 'learning_rate': 3.380952380952381e-05, 'epoch': 16.82}\n",
      "{'loss': 1.8048, 'grad_norm': 1.680277705192566, 'learning_rate': 3.3761904761904764e-05, 'epoch': 16.87}\n",
      "{'loss': 1.8445, 'grad_norm': 1.5003960132598877, 'learning_rate': 3.3714285714285716e-05, 'epoch': 16.92}\n",
      "{'loss': 1.8077, 'grad_norm': 1.4400677680969238, 'learning_rate': 3.366666666666667e-05, 'epoch': 16.96}\n",
      "{'loss': 1.894, 'grad_norm': 1.527470588684082, 'learning_rate': 3.361904761904762e-05, 'epoch': 17.01}\n",
      "{'loss': 1.7297, 'grad_norm': 1.506737232208252, 'learning_rate': 3.357142857142857e-05, 'epoch': 17.06}\n",
      "{'loss': 1.7492, 'grad_norm': 1.629474401473999, 'learning_rate': 3.352380952380952e-05, 'epoch': 17.1}\n",
      "{'loss': 1.7743, 'grad_norm': 1.646329402923584, 'learning_rate': 3.347619047619048e-05, 'epoch': 17.15}\n",
      "{'loss': 1.8159, 'grad_norm': 1.6750071048736572, 'learning_rate': 3.342857142857143e-05, 'epoch': 17.2}\n",
      "{'loss': 1.8, 'grad_norm': 1.5781941413879395, 'learning_rate': 3.338095238095238e-05, 'epoch': 17.24}\n",
      "{'loss': 1.7334, 'grad_norm': 1.5272881984710693, 'learning_rate': 3.3333333333333335e-05, 'epoch': 17.29}\n",
      "{'loss': 1.7312, 'grad_norm': 1.480751872062683, 'learning_rate': 3.3285714285714286e-05, 'epoch': 17.34}\n",
      "{'loss': 1.7902, 'grad_norm': 1.544655442237854, 'learning_rate': 3.323809523809524e-05, 'epoch': 17.38}\n",
      "{'loss': 1.7915, 'grad_norm': 1.4814077615737915, 'learning_rate': 3.3190476190476196e-05, 'epoch': 17.43}\n",
      "{'loss': 1.7918, 'grad_norm': 1.4991329908370972, 'learning_rate': 3.314285714285714e-05, 'epoch': 17.48}\n",
      "{'loss': 1.7408, 'grad_norm': 1.5561199188232422, 'learning_rate': 3.309523809523809e-05, 'epoch': 17.52}\n",
      "{'loss': 1.8363, 'grad_norm': 1.4978177547454834, 'learning_rate': 3.304761904761905e-05, 'epoch': 17.57}\n",
      "{'loss': 1.7603, 'grad_norm': 1.6407402753829956, 'learning_rate': 3.3e-05, 'epoch': 17.62}\n",
      "{'loss': 1.7109, 'grad_norm': 1.595537543296814, 'learning_rate': 3.2952380952380954e-05, 'epoch': 17.66}\n",
      "{'loss': 1.792, 'grad_norm': 1.5638004541397095, 'learning_rate': 3.2904761904761906e-05, 'epoch': 17.71}\n",
      "{'loss': 1.7552, 'grad_norm': 1.572723388671875, 'learning_rate': 3.285714285714286e-05, 'epoch': 17.76}\n",
      "{'loss': 1.7863, 'grad_norm': 1.5367960929870605, 'learning_rate': 3.2809523809523815e-05, 'epoch': 17.8}\n",
      "{'loss': 1.7296, 'grad_norm': 1.519943356513977, 'learning_rate': 3.276190476190477e-05, 'epoch': 17.85}\n",
      "{'loss': 1.7251, 'grad_norm': 1.6391332149505615, 'learning_rate': 3.271428571428571e-05, 'epoch': 17.9}\n",
      "{'loss': 1.7499, 'grad_norm': 1.5475828647613525, 'learning_rate': 3.266666666666667e-05, 'epoch': 17.94}\n",
      "{'loss': 1.7924, 'grad_norm': 1.6170310974121094, 'learning_rate': 3.261904761904762e-05, 'epoch': 17.99}\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results2',          # output directory\n",
    "    num_train_epochs=50,              # total number of training epochs\n",
    "    per_gpu_train_batch_size=64,   # batch size for training\n",
    "    per_gpu_eval_batch_size=64,    # batch size for evaluation\n",
    "    warmup_steps=100,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    logging_steps=5,\n",
    "    report_to=\"none\", # disable Wandb reporting\n",
    "    use_cpu=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets,\n",
    "    #eval_dataset=tokenized_val_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'early_stopping': True, 'num_beams': 4, 'no_repeat_ngram_size': 3, 'forced_eos_token_id': 2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(\"C:\\\\Users\\\\me513\\\\Downloads\\\\balance\\\\'50امراض المسالك البولية والتناسلية'\\\\tokenizer_config.json\",\n",
       " \"C:\\\\Users\\\\me513\\\\Downloads\\\\balance\\\\'50امراض المسالك البولية والتناسلية'\\\\special_tokens_map.json\",\n",
       " \"C:\\\\Users\\\\me513\\\\Downloads\\\\balance\\\\'50امراض المسالك البولية والتناسلية'\\\\sentencepiece.bpe.model\",\n",
       " \"C:\\\\Users\\\\me513\\\\Downloads\\\\balance\\\\'50امراض المسالك البولية والتناسلية'\\\\added_tokens.json\",\n",
       " \"C:\\\\Users\\\\me513\\\\Downloads\\\\balance\\\\'50امراض المسالك البولية والتناسلية'\\\\tokenizer.json\")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = r\"C:\\Users\\me513\\Downloads\\balance\\50امراض الدم\"\n",
    "\n",
    "# Create the directory if it does not exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "menna",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
